\section{Critique}
This paper was published in FAST 2009, where it won the best paper award. Impressions was a project at the Advanced Systems Lab at the University of Wisconsin - Madison that's headed by the Arpaci-Dusseaus. This group holds a formidable reputation for carrying interesting and novel research in file and storage systems - a reputation that I firmly believe in \footnote{at the time of writing this section.}

Impressions was a project that was motivated by the lack of formal benchmarking practices in the space of file systems research. This is a well-identified problem, and there has been much research that has tried to address, or at the least, quantify this problem. The problem has been targeted from two different, yet synergistic directions:
\begin{itemize}
\item The creation of more \emph{representative workloads} for benchmarking experiments. This involves the study of access patterns on production filesystems and creating workload generators that mimic these access patterns. This appears to be a crowded research space, as can be seen by the sheer number of benchmarks studied by ~\cite{tarasov2011benchmarking}.
\item Creating more representative filesystem state on which to run benchmarks. This is a combination of the memory state, the on-disk layout/ fragmentation of the filesystem, and the metadata of the file-system image itself. The cache effects and fragmentation effects have been well studied in prior research. The file-system images used for these studies have been based on ad-hoc and inaccurate assumptions. This is the central issue that this work seeks to address
\end{itemize}

The authors point out filesystem researchers rely upon non-standard and often arbitrary filesystem images to test the performance of their system, despite there being an abundance of empirical studies of file system contents and metadata. It is, therefore, difficult to verify the performance of the filesystem in a way that is demonstrably independent of the effects that can arise from the actual file-system image used for evaluation.

Impressions is a framework to generate \emph{representative, and statistically accurate file system images while ensuring complete reproducibility of the image}. The framework uses a lot of statistical methods to create the image, and the parameters used to create this image can be tweaked. Also, given the key parameters, it becomes trivial to reproduce the test image, thereby ensuring easy and robust reproducibility.


\subsection{Experimental Setup} There is no experimental setup that is explicitly mentioned in the paper. This is very surprising to me since they mention the time it takes to complete desktop search on the generated image. Without any information about the machine and the software on which this experiment was run, it becomes a little difficult to believe and reproduce their results.
There is only one explicit system information that is mentioned which is the filesystem that was used to create the generated system on. They mention that they rely on \emph{debugfs} for identifying on-disk layout which is used to modulate fragmentation of the generated filesystem (ext2 or ext3). Even this fact is hidden deep in the section on Disk Layout and Fragmentation, and not explicitly stated as should have been the case. 

\subsection{Research Execution}\label{sssec:re} The research methodology in this paper is quite sketchy. I think that the methodology design was very elegant, but at the same time, it was presented in a very unsatisfactory manner. I have the following issues with their research execution:
\begin{enumerate}[nosep]
    \item In Figure 1, they mention the difference in relative time taken for \texttt{find /} on images with different on-disk layouts (fragmentation). The only details about this experiment are mentioned in the figure label. This is the only experiment where they actually demonstrate the effect of fragmentation on the filesystem image, and yet they do not explicitly mention the filesystem(ext2 or ext3), or any other system information that is needed to reproduce this result. 
    \item Table 2 presents the \emph{default} values that the impressions framework assumes. While this is great from a theoretical point-of-view, this does not map well to the input file that the framework uses to accept these parameters. There is scant documentation provided to explain those parameters. While this does not impact the validity of the results - the inputfile shipped with the tarball has all the params already tuned to generate the filesystem image used for the experiments in figure 2.
    \item They determine the goodness of their simulation by comparing how closely they can recreate the metrics from the dataset in ~\cite{agrawal2007five}. This is a good metric for this work. However, it does not reliably convey the effort involved in finding the parameters to make the generative model fit so perfectly with the desired model. 
    \item The dataset in ~\cite{agrawal2007five} is based on traces from \emph{Windows PC} machines that were, as the authors pointed, drawn from a homogenous sample. This is not generalizable to other filesystems or even to other workloads (example: a webserver workload). This makes me feel like this paper reports the authors' experience in making the generative model match an arbitrary dataset, and this eclipses the claim of representativeness.
\end{enumerate}
